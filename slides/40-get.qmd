---
title: "Get Data into R"
subtitle: "Tidyverse - Part 3"
author: "Otho Mantegazza"
execute: 
  echo: true
format:
  revealjs:
    theme: [simple, style/theme.scss]
    incremental: true
    footer:  '[Home](../index.html) | [2022 CBSER Summer School](https://www.summercompschool.mawazoinstitute.org/)'
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: false
library(dplyr)
library(readr)
library(magrittr)
library(tidyr)
library(palmerpenguins)
library(tibble)
library(ggplot2)
library(readr)
library(here)
```

# Datasets

## Dataframes are rectangular datasets

More often than not when we speak about **datasets**, we speak about rectangular data, i.e., data in **two-dimensional table**, made of values organized in rows and columns.

. . .

- Each **cell** stores a value.
- Each value belongs to one **column** and one **row**.

. . .

Rectnagular data are the easiest to use, when we get data that are not rectangular, we try to reshape them in that form.

# Tools: Read Data with Readr

## Readr

:::: {.columns}

::: {.column width="40%"}
![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/SVG/readr.svg)
:::

::: {.column width="60%"}

[Readr](https://readr.tidyverse.org/) is a package that loads (reads) **Rectangular Text** data in R.

It's fast, it guesses column types explicitly and it's pipe friendly

You can use it to read both local data and online data from a URL.

For example we can use it to read data in CSV and TSV formats and many more.

:::

::::

## Read the Palmer Pengunis dataset {.scrollable}

We can use again on the [Palmer Penguins Dataset](https://allisonhorst.github.io/palmerpenguins/)

The [source code of this package](https://github.com/allisonhorst/palmerpenguins/), is on github; we can find the [tidy CSV data](https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv) in the [inst/exdata folder](https://github.com/allisonhorst/palmerpenguins/tree/main/inst/extdata).

. . .

```{r}
#| message: true
penguin_csv_url <- 'https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv'
 
read_csv(penguin_csv_url)
```

## {.scrollable}

The tibble that we have loaded and generate d from CSV is not identical to the one that comes already loaded with the `palmerpenguins` package:

```{r}
penguins_from_csv <- 
  penguin_csv_url %>% 
  read_csv()

identical(
  penguins_from_csv,
  palmerpenguins::penguins
)
```

Let's compare them side by side

:::: {.columns}

::: {.column width="50%"}

```{r}
palmerpenguins::penguins %>% 
  glimpse(width = 40)
```

:::

::: {.column width="50%"}

```{r}
penguins_from_csv %>% 
  glimpse(width = 40)
```

:::

::::

Can you spot that column types are different?

<br>
<br>

## Parsing {.smallish .scrollable}

When we read data from text encoded "delimited" files, such as CSV, we use function that [parse](https://en.wikipedia.org/wiki/Parsing) the file.

When we parse something, we formalize its structure applying a set of grammatical rules.

No parsing rule is perfect, thus we must often review the results and "fix" parsing "mistakes".

```{r}
# specify column types manually

penguins_from_csv <-
  penguin_csv_url %>% 
  read_csv(
    col_types = cols(
      species = col_factor(),
      island = col_factor(),
      flipper_length_mm = col_integer(),
      body_mass_g = col_integer(),
      sex = col_factor(),
      year = col_integer()
    )
  )

penguins_from_csv %>% glimpse()
```

## Exercise {.exercise}

Find the source code of the [readr package](https://readr.tidyverse.org/).

In the `inst/extdata` folder you can find 10 datasets that display different challenges that you might enconter when you have to load data from an external file.

Load in R at least 3 of those datasets using functions from `readr`.

Get help from [readr's documentation](https://readr.tidyverse.org/) and the [data import chapter](https://r4ds.had.co.nz/data-import.html) of r4ds.

Which function did you use? Did you encounter any parsing failure? How did you fix it?


## Read a dataset from PANGAEA 

[PANGAEA](https://www.pangaea.de/), a Data repository for the evironmental sciences.

For our exercise we will use [this dataset](https://doi.pangaea.de/10.1594/PANGAEA.946258) from Wu et al: 

<br>

::: {.big}

"Effect of barite-bound Sr on detrital Sr isotope systematics in marine sediments with pertinent Mediterranean examples".

:::

<br>

[https://doi.pangaea.de/10.1594/PANGAEA.946258](https://doi.pangaea.de/10.1594/PANGAEA.946258)

## {}

```{r}
pangaea_filename <- 'Dataset_S2_HCl-leaching.tab'

pangaea_path <- here('data-int/Wu-etal_2022/datasets', pangaea_filename)
```

Let's try to read the data file `r pangaea_filename`.

It's a `.tab` file. 

```{r}
#| warning: true

pangaea_data <- 
  pangaea_path %>% 
  read_delim()
```

## {}

If we call `problems()` readr tells us what went wrong.

```{r}
pangaea_data
```

## {}

We can use the arguments:

::: {.nonincremental}

- `delim = '\t'` to tell `read_delim()` that we are reading a file delimited by tabulature (\\t).
- `skip = 49` to tell it that the first 49 rows must be skipped.

:::

```{r}
pangaea_data <- 
  pangaea_path %>% 
  read_delim(delim = '\t',
             skip = 49)
```

## {}

Now the data that we've imported into R looks fine.

```{r}
pangaea_data %>% glimpse()
```


## {.scrollable}

Now the data that we've imported into R looks fine.

```{r}
#| output: text
pangaea_data %>% skimr::skim()
```


## Always check for missing values

`skimr::skim()` shows you how many values are missing in your dataset:

::: {.nonincremental}

- How many missing value are there?
- Where do they occur?

:::

## {.scrollable}

A more formal way to check for missing values.

```{r}
pangaea_data %>% 
  summarise(
    across(
      .fns = ~is.na(.) %>% sum()
    )
  ) %>% 
  glimpse()
```

## Quick checklist when you read new data into R

::: {.nonincremental}

- Check for missing values.
- Check the column types, are they what you expect?
- Check the row number and the column names.
- Optional, check the `head()` and the `tail()` of the file.

:::

Now let's tidy the data.

## Exercise {.exercise}

[Tidytuesday](https://github.com/rfordatascience/tidytuesday) is a weekly data project aimed at learning, collaborating and networking the R ecosystem.

Find this week's dataset and read it in R. Run the checklist from the previous slide on the data that you've read.

If you are donw early, proceed reading data from the previous week or find a colleague to help.

Check Tidytuesday submissions on [Twitter with the hashtag #TidyTuesday](https://twitter.com/hashtag/TidyTuesday)

