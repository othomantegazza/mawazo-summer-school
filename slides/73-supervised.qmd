---
title: "A Glimpse to Linear Models"
subtitle: '[Home](../index.html) | [CBSER Summer School by Mawazo Institute](https://www.summercompschool.mawazoinstitute.org/)'
author: "Otho Mantegazza"
execute: 
  echo: true
format:
  html:
    toc: true
    number-sections: true
editor_options: 
  chunk_output_type: console
---

# Linear Models

Linear model are historically the first type of supervised model that is encountered when studying statistical learning, and are still probably the most used in biology, ecology and plenty of other fields. The most simple form of linear model is linear regression.

You can use linear model to test if a combination of one or more predictor variables can "explain", infer or predict a response variable.

For reference on linear models check:

-   Chapters 3 to 7 of [ISLR](https://www.statlearning.com).
-   Chapters 16, 17 and 18 of [Introduction to Data Science](http://rafalab.dfci.harvard.edu/dsbook/regression.html).
-   [Analising Data Using Linear Models](https://bookdown.org/pingapang9/linear_models_bookdown/), for students in social, behavioural and management science, by St√©phanie M. van den Berg, basically the whole book.

Let's try linear regression in R.

# Packages

First let's load a selection of packages that we have been using throughout the lessons.

```{r}
#| message: false
library(dplyr)
library(readr)
library(magrittr)
library(tidyr)
library(tibble)
library(ggplot2)
library(here)
```

Then, let's load a couple of packages specific for statistical modeling from the [tidymodels](https://www.tidymodels.org/) framework.

```{r}
#| message: false
library(broom)
library(parsnip)
```

And let's tweak the ggplot theme so graphics are easier to read on this page.

```{r}
theme_update(axis.title = element_text(size = 15),
             axis.text = element_text(size = 15),
             strip.text = element_text(size = 15))
```

# The rice dataset

We will keep working on the [rice dataset](https://academic.oup.com/jxb/article/70/20/5617/5538968), introduced in the [slides on explorative data analysis](60-explorative-data-analysis.html#/practice-dataset).

## Load and check

### Read the data

I've mirrored the data on my instance of github, let's read them from there.

```{r}
rice <-  
  paste0(
    'https://raw.githubusercontent.com/othomantegazza',
    '/mawazo-summer-school/main/data-int/rice.csv'
  ) %>% 
  read_delim(delim = ';') %>% 
  janitor::clean_names()
```

### Structure of the data

Let's also check how the data look like:

```{r}
rice
```

```{r}
rice %>% glimpse()
```

```{r}
rice %>% count(species)
```

```{r}
rice %>% pull(accession_name) %>% n_distinct()
```

### Content and aim of the data

The dataset contains phenotypic measurements of 1140 rice panicle from 91 rice accessions of 4 different rice species.

The aim of [the paper in which this dataset is published](https://academic.oup.com/jxb/article/70/20/5617/5538968) is to compare panicles at a phenotypic and transcriptomic level for a combination of 4 African and Asian, wild and domesticated rice species.

```{r}
#| echo: false
tribble(
  ~species_id, ~species, ~continent, ~status,
   'Ob', 'Oryza barthii', 'African', 'Wild',
   'Og', 'Oryza glaberrima', 'African', 'Cultivated',
   'Or', 'Oryza rufipogon', 'Asian', 'Wild',
   'Os', 'Oryza sativa', 'Asian', 'Cultivated'
) %>% knitr::kable()
```

In the picture below, from the [original article (*CC-BY-NC*)](https://academic.oup.com/jxb/article/70/20/5617/5538968) you can find a graphical explanation of the phenotypic features measured in the rice dataset.

![](img/rice-panicle.png)

## Question

Let's try to answer the question: Can we use the number of branches (PB e SB) on a panicle to explain the number of spikelets (SP) on the same panicle? In case how?

## Visualize the data first

To answer the question, let's first visualize the data to get an idea of how they are distributed.

```{r}
rice %>% 
  ggplot(aes(x = primary_branch_number_pbn,
             y = spikelet_number_sp_n,
             colour = species)) +
  geom_point(alpha = .4) 
```

It seems that the number of spikelets grows with the number of primary branches on a panicle. The pattern might not be perfectly linear, and there might be some variation among different rice species.

Let's also visualize the same data divided in facets, to get a clearer idea of how different species behave.

```{r}
rice %>% 
  ggplot(aes(x = primary_branch_number_pbn,
             y = spikelet_number_sp_n,
             colour = species)) +
  geom_point(alpha = .4) +
  facet_wrap(facets = 'species')
```

The data points are more dispersed vertically in Og (Oryza glaberrima) and Oryza sativa. Those two species are domesticated species, they produce many secondary branches, this "uncouples" the number of spikelets from the number of primary branches.

Can we describe this observation with a statistical model using linear regression?

# A syntax for models

How do we define a model in R? Let's start from a generic regression.

## Generic linear regression

In a simple linear model, such as linear regression, we are modelling the response variable `Y` as a combination two coefficients `a` and `b` that define the intercept and the slope of the line, the predictor `X` and an `error` term that, in absence of a better explanation, we hypothesize as being random gaussian.

$$Y = a + b * X + error$$

We can express the same concept with a tilde `~` formula.

$$Y \sim a + b * X$$ If we substitute the generic terms of the equation above with the variables from the rice dataset, we can say that, in the rice problem, we are trying to guess the `intercept` and `slope` coefficients for this equation.

$$spikelets = intercept + slope * primary\_branches + error$$ Let's use a tilde formula again.

$$spikelets \sim intercept + slope * primary\_branches$$

With this syntax we can express models with any number of predictors, or any mathematical combination of them.

$$Y = a + b_1X_1 + b\_2X_2 + ... + b\_nX_n + error$$ \## Model formulas in R

R has adopted the [tilde model formula definition](https://thomasleeper.com/Rcourse/Tutorials/formulae.html) that we have seen above to define models within it's function, such as `lm()`. So, for example, the formula: `spikelet_number_sp_n ~ primary_branch_number_pb` will use:

-   The variable `spikelet_number_sp_n` as response.
-   The `primary_branch_number_pbn` as predictor.
-   The intercept and the slope get's calculated automatically when we provide this formula and the associated data to the function `lm()`

# Run the model

We can declare and evaluate a linear model in R with the function `lm()`.

Let's run it with the formula mentioned above on the `rice` data.

```{r}
fit <-
  lm(
    formula = spikelet_number_sp_n ~ primary_branch_number_pbn,
     data = rice
    )
```

We have run a linear regression of `spikelet_number_sp_n` over `primary_branch_number_pbn`. And associated the results to the object `fit`. How do we see the results?

Just by calling the object fit, we can print on screen the slope and intercept estimated by the model.

```{r}
fit
```

In the next sessions we will explore the many available tools to get more information and diagnostics from the model.

## Explore the results

### A summary of the results

The [`summary()`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/summary.lm) method prints:

-   The **coefficients** that we've seen above (in this case the slope and the intercept of the linear regression line) with the associated T-statistics.
-   **R-squared** and **F-statistics** measuring and scoring the overall model.

```{r}
fit %>% summary()
```

### Diagnostic plots {#diagnostic-plots}

If you use the [`plot()`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/plot.lm) method on a fitted `lm` object, R prints a selection of [4 diagnostic plots](https://www.r-bloggers.com/2019/11/the-hidden-diagnostic-plots-for-the-lm-object/).

```{r}
#| fig-height: 7
par(mfrow = c(2, 2)) # organize the plots in a 2x2 grid
plot(fit)
```

## Tidy the results with `broom`

To access the coefficients and statistics of the model systematically, what better choice than to tidy them in a tibble with the package [broom](https://broom.tidymodels.org/)

::: columns
::: {.column width="40%"}
![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/SVG/broom.svg)
:::

::: {.column width="10%"}
:::

::: {.column width="50%"}
[broom](https://broom.tidymodels.org/) summarizes key information about models in tidy `tibble()`s. broom provides three verbs to make it convenient to interact with model objects:

-   `tidy()` summarizes information about model components.
-   `glance()` reports information about the entire model.
-   `augment()` adds information about observations to a dataset.
:::
:::

Let's check broom's main functions.

### Glance the results of the linear regression {.scrollable}

`glance()` reports information about the entire model, similar to what we have seen in the last lines of the output of `summary()`, but in a tibble.

```{r}
fit %>%
  glance() %>% 
  glimpse()
```

### Tidy the results of the linear regression {.scrollable}

`tidy()` summarizes information about model coefficients and their statistics in a tibble.

```{r}
fit_tidy <-
  fit %>%
  tidy()

fit_tidy
```

### Augment the results of the linear regression {.scrollable}

`augment()` returns a tibble with the fitted and residuals values for every observation that was used to train the model.

```{r}
fit_augmented <-
  fit %>%
  augment()

fit_augmented
```

## Visualize the results of the linear regression

Once we have extracted the model coefficients with broom, we can use ggplot to represent the regression line graphically.

```{r}

mod_intercept <- fit_tidy %>% slice(1) %>% pull(estimate)
mod_slope <- fit_tidy %>% slice(2) %>% pull(estimate)

rice %>% 
  ggplot(aes(x = primary_branch_number_pbn,
             y = spikelet_number_sp_n)) +
  geom_point(alpha = .2) +
  geom_abline(intercept = mod_intercept,
              slope = mod_slope)
```

It's basically the same result that we get if we add a `geom_smooth()` to the graphic, with the parameter `method = 'lm'`.

```{r}
rice %>% 
  ggplot(aes(x = primary_branch_number_pbn,
             y = spikelet_number_sp_n)) +
  geom_point(alpha = .2) +
  geom_smooth(method = 'lm')
```

## Conclusions from the model and from the graph

-   The model suggests that the number of spikelets grows with the number of primary branches. More precisely, the model estimates that on average, you get `r mod_slope %>% round(2)` spikelets every new primary branch.
-   We can see visually that the variance of the response variable `spikelet_number_sp_n` grows when the predictor `primary_branch_number_pbn` grows. So at high number of primary branches we can expect our predictions of spikelet number to be more approximated.
-   You can see from the [diagnostic plot](#diagnostic-plots), that the relationship between number of spikelets and primary branches might not be perfectly linear. One educated hypothesis to explain this behaviour might be that the species that produce most spikelets rely heavily on secondary branches, and not only on primary branches.

# Multiple regression

## What if we want to use more than one predictor

We can specify any type of formula, including as predictor both continuous and categorical variables or any type of mathematical transformation of such variables.

See chapter 3 and 7 of [Introduction to Statistical Learning](https://www.statlearning.com/) to learn in detail about multiple regression and moving beyond linearity.

Let's try to fit the spikelet number as a function of both primary and secondary branch number. To achieve this we can add the variable `secondary_branch_number_sbn` to the model formula that we have seen above.

`spikelet_number_sp_n ~ primary_branch_number_pbn + secondary_branch_number_sbn`

```{r}
fit_multiple <- lm(
  formula = spikelet_number_sp_n ~
    primary_branch_number_pbn +
    secondary_branch_number_sbn,
  data = rice)
```

Let's explore the results of the fit:

```{r}
fit_multiple %>%
  glance() %>% 
  glimpse()
```

```{r}
fit_multiple %>% 
  tidy()
```

The p-value of the new term `secondary_branch_number_sbn` has an incredibly low p-value (approximated at 0), a hint that the this term is adding information to the model and is helping us building a better function that connects the spikelet number to the branch number.

We can use the diagnostic plot to check how this model is performing.

```{r}
#| fig-height: 7
par(mfrow = c(2, 2)) # organize the plots in a 2x2 grid
fit_multiple %>% plot()
```

You can extend this reasoning to any variable in the dataset.

For example, try yourself to model the number of spikelet with this formula below, in which we add the variable as a new predictor `species`:

    spikelet_number_sp_n ~
       primary_branch_number_pbn +
       secondary_branch_number_sbn +
       species

## Comparing models

As our model become more and more nuanced and complicated, it gets harder to understand the output intuitively, and to visualize the results.

Thus it gets harder to answer vital questions such as:

-   which combination and transformation of predictors should I include in the model,
-   which model is best to encapsulate the signal in my data, successfully excluding the noise.

Predictor and model selection is an incredibly important and wide field in modelling and the method might change whether you're aim is inference or prediction.

A very basic example of ways to compare similar model is to run an anova to check if the model with an added term is explaining more variance then the simpler one.

```{r}
anova(
  fit, 
  fit_multiple
)
```

But you can learn more effective ways to select and test if your linear model is working properly in:

-   Chapter 5 "Resampling Methods" and chapter 6 "Linear Model Selection and Regularization" of [Introduction to Statistical Learning](https://www.statlearning.com/).
-   Chapter 10 to 15 of [Tidy Modeling with R](https://www.tmwr.org/).

# Tidymodels

We had a glimpse of what you can do in R with linear models.

The main function modeling function that we have used is `lm()` from the [`stats`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html) package (which is loaded in R at start up, and that's why we didn't have to load it calling `library(stats)`).

R has many other packages dedicated to modeling, provided access to a multitude of advanced modeling methods, each with a dedicated interface and diagnostic tools.

To unify and ease access to this multitude and to encourage good modeling practices through suggested and standardized workflow, the R community recently published the [`tidymodels`](https://www.tidymodels.org/) framework.

Just as we load all tidyverse packages at once by calling `library(tidyverse)`, we can load every tidymodels package calling `library(tidymodels)` (remember that first you have to install them with `install.packages("tidymodels")`).

We have already used the `broom` package from tidymodels, installing it and loading it separately from the whole framework.

## Example with linear regression

Let's load also the tidymodels package [parsnip](https://parsnip.tidymodels.org/) and use it to fit the same linear regression that we have seen above.

```{r}
library(parsnip)
```

First let's define a linear regression model and declare that we are using `lm()` as the modelling engine.

```{r}
mod <-
  linear_reg() %>%
  set_engine('lm')
```

Then, let's fit the model

```{r}
mod_fit <- 
  mod %>% 
  fit(
    formula = spikelet_number_sp_n ~ primary_branch_number_pbn,
    data = rice
  )

tidy(mod_fit)
```

## A unified framework

We have achieved the same as when we where calling `lm()`. Although we have used more lines of code, if you get used to the tidymodels syntax, you will have access to advanced modeling methods and workflows easily, because the same standardized syntax will apply to all modeling methods that you want to apply.

Check the [Tidymodels with R](https://www.tmwr.org/index.html) book to take your next step in the statistical modeling world with R.
